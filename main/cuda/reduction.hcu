#if !defined(_REDUCTION_HCU_)
#define _REDUCTION_HCU_

#include "fma.hcu"
#include "vectorMin.hcu"

template<const int T1, const int T2>
__device__ __inline__ void _dev_reduction(const int r, const int c, double2 * Creg,
					  double2 A[TILESIZEY][T1],
					  double2 B[TILESIZEY][T2],
					  double * __restrict__ C,
					  int * __restrict__ Cindices) {
}

template<const int T1, const int T2>
__device__ __inline__ void _dev_reduction(const int r, const int c, double4 * Creg,
					  double4 A[TILESIZEY][T1],
					  double4 B[TILESIZEY][T2],
					  double * __restrict__ C,
					  int * __restrict__ Cindices) {
  
}


template<const int T1, const int T2>
__device__ __inline__ void _dev_reduction(const int r, const int c, float2 * Creg,
					  float2 A[TILESIZEY][T1],
					  float2 B[TILESIZEY][T2],
					  float * __restrict__ C,
					  int * __restrict__ Cindices) {

}

template<const int T1, const int T2>
__device__ __inline__ void _dev_reduction(const int r, const int c, float4 * Creg,
					  float4 A[TILESIZEY][T1],
					  float4 B[TILESIZEY][T2],
					  float * __restrict__ C,
					  int * __restrict__ Cindices) {

  int4 i;
  int4 j;
  float4 s, t;

  vectorMax<float,float4>(Creg[0],s.x,i.x);
  vectorMax<float,float4>(Creg[1],s.y,i.y);
  vectorMax<float,float4>(Creg[2],s.z,i.z);
  vectorMax<float,float4>(Creg[3],s.w,i.w);

  i.x+=c;
  i.y+=c;
  i.z+=c;
  i.w+=c;

  for (int n=1; n<TILESIZE; n<<=1) {
    t.x = __shfl_down(s.x, n, TILESIZE);
    j.x = __shfl_down(i.x, n, TILESIZE);

    t.y = __shfl_down(s.y, n, TILESIZE);
    j.y = __shfl_down(i.y, n, TILESIZE);

    t.z = __shfl_down(s.z, n, TILESIZE);
    j.z = __shfl_down(i.z, n, TILESIZE);

    t.w = __shfl_down(s.w, n, TILESIZE);
    j.w = __shfl_down(i.w, n, TILESIZE);

    if ((threadIdx.x%(n*2))==0) {
      if (s.x<t.x) { i.x = j.x; s.x=t.x; }
      if (s.y<t.y) { i.y = j.y; s.y=t.y; }
      if (s.z<t.z) { i.z = j.z; s.z=t.z; }
      if (s.w<t.w) { i.w = j.w; s.w=t.w; }
    }
  }

  t.w = __shfl_up(s.w, 3, TILESIZE);
  j.w = __shfl_up(i.w, 3, TILESIZE);

  t.z = __shfl_up(s.z, 2, TILESIZE);
  j.z = __shfl_up(i.z, 2, TILESIZE);

  t.y = __shfl_up(s.y, 1, TILESIZE);
  j.y = __shfl_up(i.y, 1, TILESIZE);

  t.x = s.x;
  j.x = i.x;
  
  float * a = reinterpret_cast<float *>(&t);
  int * b = reinterpret_cast<int *>(&j);
    
  /* write out the results */
  if (threadIdx.x<4) {
    int k = r + threadIdx.x;
    if (k<dev_nRowsA) {
      int index = k*dev_nColsC+blockIdx.x;
      C[index]        = -2.0f*a[threadIdx.x];
      Cindices[index] = b[threadIdx.x];
    }
  }
}

template<const int T1, const int T2>
__device__ __inline__ void _dev_reduction(const int r, const int c, float6 * Creg,
					  float6 A[TILESIZEY][T1],
					  float6 B[TILESIZEY][T2],
					  float * __restrict__ C,
					  int * __restrict__ Cindices) {

  int6 i;
  int6 j;
  float6 s, t;

  vectorMax<float,float6>(Creg[0],s.a,i.a);
  vectorMax<float,float6>(Creg[1],s.b,i.b);
  vectorMax<float,float6>(Creg[2],s.c,i.c);
  vectorMax<float,float6>(Creg[3],s.d,i.d);
  vectorMax<float,float6>(Creg[4],s.e,i.e);
  vectorMax<float,float6>(Creg[5],s.f,i.f);

  i.a+=c;
  i.b+=c;
  i.c+=c;
  i.d+=c;
  i.e+=c;
  i.f+=c;
  
  for (int n=1; n<TILESIZE; n<<=1) {
    t.a = __shfl_down(s.a, n, TILESIZE);
    j.a = __shfl_down(i.a, n, TILESIZE);

    t.b = __shfl_down(s.b, n, TILESIZE);
    j.b = __shfl_down(i.b, n, TILESIZE);

    t.c = __shfl_down(s.c, n, TILESIZE);
    j.c = __shfl_down(i.c, n, TILESIZE);

    t.d = __shfl_down(s.d, n, TILESIZE);
    j.d = __shfl_down(i.d, n, TILESIZE);

    t.e = __shfl_down(s.e, n, TILESIZE);
    j.e = __shfl_down(i.e, n, TILESIZE);

    t.f = __shfl_down(s.f, n, TILESIZE);
    j.f = __shfl_down(i.f, n, TILESIZE);

    if ((threadIdx.x%(n*2))==0) {
      if (s.a<t.a) { i.a = j.a; s.a=t.a; }
      if (s.b<t.b) { i.b = j.b; s.b=t.b; }
      if (s.c<t.c) { i.c = j.c; s.c=t.c; }
      if (s.d<t.d) { i.d = j.d; s.d=t.d; }
      if (s.e<t.e) { i.e = j.e; s.e=t.e; }
      if (s.f<t.f) { i.f = j.f; s.f=t.f; }
    }
  }

  t.f = __shfl_up(s.f, 5, TILESIZE);
  j.f = __shfl_up(i.f, 5, TILESIZE);

  t.e = __shfl_up(s.e, 4, TILESIZE);
  j.e = __shfl_up(i.e, 4, TILESIZE);

  t.d = __shfl_up(s.d, 3, TILESIZE);
  j.d = __shfl_up(i.d, 3, TILESIZE);

  t.c = __shfl_up(s.c, 2, TILESIZE);
  j.c = __shfl_up(i.c, 2, TILESIZE);

  t.b = __shfl_up(s.b, 1, TILESIZE);
  j.b = __shfl_up(i.b, 1, TILESIZE);

  t.a = s.a;
  j.a = i.a;

  float * a = reinterpret_cast<float *>(&t);
  int * b = reinterpret_cast<int *>(&j);

  /* write out the results */
  if (threadIdx.x<6) {
    int k = r+threadIdx.x;
    if (k<dev_nRowsA) {
      int index = k*dev_nColsC+blockIdx.x;
      C[index]        = -2.0f*a[threadIdx.x];
      Cindices[index] = b[threadIdx.x];
    }
  }
}



template<const int T1, const int T2>
__device__ __inline__ void _dev_reduction(const int r, const int c, float8 * Creg,
					  float8 A[TILESIZEY][T1],
					  float8 B[TILESIZEY][T2],
					  float * __restrict__ C,
					  int * __restrict__ Cindices) {

  int8 i;
  int8 j;
  float8 s, t;

  vectorMax<float,float8>(Creg[0],s.a,i.a);
  vectorMax<float,float8>(Creg[1],s.b,i.b);
  vectorMax<float,float8>(Creg[2],s.c,i.c);
  vectorMax<float,float8>(Creg[3],s.d,i.d);
  vectorMax<float,float8>(Creg[4],s.e,i.e);
  vectorMax<float,float8>(Creg[5],s.f,i.f);
  vectorMax<float,float8>(Creg[6],s.g,i.g);
  vectorMax<float,float8>(Creg[7],s.h,i.h);

  i.a+=c;
  i.b+=c;
  i.c+=c;
  i.d+=c;
  i.e+=c;
  i.f+=c;
  i.g+=c;
  i.h+=c;
  
  for (int n=1; n<TILESIZE; n<<=1) {
    t.a = __shfl_down(s.a, n, TILESIZE);
    j.a = __shfl_down(i.a, n, TILESIZE);

    t.b = __shfl_down(s.b, n, TILESIZE);
    j.b = __shfl_down(i.b, n, TILESIZE);

    t.c = __shfl_down(s.c, n, TILESIZE);
    j.c = __shfl_down(i.c, n, TILESIZE);

    t.d = __shfl_down(s.d, n, TILESIZE);
    j.d = __shfl_down(i.d, n, TILESIZE);

    t.e = __shfl_down(s.e, n, TILESIZE);
    j.e = __shfl_down(i.e, n, TILESIZE);

    t.f = __shfl_down(s.f, n, TILESIZE);
    j.f = __shfl_down(i.f, n, TILESIZE);

    t.g = __shfl_down(s.g, n, TILESIZE);
    j.g = __shfl_down(i.g, n, TILESIZE);

    t.h = __shfl_down(s.h, n, TILESIZE);
    j.h = __shfl_down(i.h, n, TILESIZE);

    if ((threadIdx.x%(n*2))==0) {
      if (s.a<t.a) { i.a = j.a; s.a=t.a; }
      if (s.b<t.b) { i.b = j.b; s.b=t.b; }
      if (s.c<t.c) { i.c = j.c; s.c=t.c; }
      if (s.d<t.d) { i.d = j.d; s.d=t.d; }
      if (s.e<t.e) { i.e = j.e; s.e=t.e; }
      if (s.f<t.f) { i.f = j.f; s.f=t.f; }
      if (s.g<t.g) { i.g = j.g; s.g=t.g; }
      if (s.h<t.h) { i.h = j.h; s.h=t.h; }
    }
  }

  t.h = __shfl_up(s.h, 7, TILESIZE);
  j.h = __shfl_up(i.h, 7, TILESIZE);

  t.g = __shfl_up(s.g, 6, TILESIZE);
  j.g = __shfl_up(i.g, 6, TILESIZE);

  t.f = __shfl_up(s.f, 5, TILESIZE);
  j.f = __shfl_up(i.f, 5, TILESIZE);

  t.e = __shfl_up(s.e, 4, TILESIZE);
  j.e = __shfl_up(i.e, 4, TILESIZE);

  t.d = __shfl_up(s.d, 3, TILESIZE);
  j.d = __shfl_up(i.d, 3, TILESIZE);

  t.c = __shfl_up(s.c, 2, TILESIZE);
  j.c = __shfl_up(i.c, 2, TILESIZE);

  t.b = __shfl_up(s.b, 1, TILESIZE);
  j.b = __shfl_up(i.b, 1, TILESIZE);

  t.a = s.a;
  j.a = i.a;

  float * a = reinterpret_cast<float *>(&t);
  int * b = reinterpret_cast<int *>(&j);

  /* write out the results */
  if (threadIdx.x<8) {
    int k = r+threadIdx.x;
    if (k<dev_nRowsA) {
      int index = k*dev_nColsC+blockIdx.x;
      C[index]        = -2.0f*a[threadIdx.x];
      Cindices[index] = b[threadIdx.x];
    }
  }
}

#endif /* !defined(_REDUCTION_HCU_) */
