#if !defined(_MULTIPLY_HCU_)
#define _MULTIPLY_HCU_

#include "zero.hcu"
#include "loadA.hcu"
#include "loadB.hcu"
#include "fma.hcu"
#include "swap.hcu"
#include "dotproducts.hcu"
#include "writeResults.hcu"


template<class TYPE, class VTYPE, const int N_UNROLL, const int DELTA>
__device__ void multiply(TYPE * __restrict__ a, TYPE * __restrict__ b, VTYPE* Creg,
			 VTYPE Ashmem[TILESIZE][TILESIZEY],
			 VTYPE Bshmem[TILESIZE][TILESIZEX]) {



  /* prefetch the first tiles */
  //VTYPE Areg, Areg1, Areg2;
  //Areg = _dev_loadA<TYPE,VTYPE>(A,0,Areg1,Areg2);

  VTYPE Areg = _dev_loadA<TYPE,VTYPE>(a);
  VTYPE Breg = _dev_loadB<TYPE,VTYPE>(b);  
  int ty = (threadIdx.y>>1) % 2;
  
  /* loop over the interior tiles */
  for (int n=1; n<=dev_N; ++n) {
    /* transfer A registers to shmem */
    Ashmem[threadIdx.y][threadIdx.x] = Areg;
    Bshmem[threadIdx.x][threadIdx.y] = Breg;    
    __syncthreads();

    /* load the next tiles */        
    a+=TILESIZE;
    b+=BSTRIDE;

    if (n<dev_N || threadIdx.x<DELTA) Areg = _dev_loadA<TYPE,VTYPE>(a);    
    //if (n<dev_N || tidx<2*DELTA) Areg = _dev_loadA<TYPE,VTYPE>(A,n,Areg1,Areg2); 
    if (n<dev_N || threadIdx.y<DELTA) Breg = _dev_loadB<TYPE,VTYPE>(b);  

    /* compute the dot products */
    _dev_dotproducts<TILESIZEY,TILESIZEX>(Ashmem,Bshmem,Creg,ty); 
    __syncthreads();
  }

  /* transfer A registers to shmem */
  Ashmem[threadIdx.y][threadIdx.x] = Areg;
  Bshmem[threadIdx.x][threadIdx.y] = Breg;
  __syncthreads();  
  
  /* compute the dot products */
  _dev_dotproductsF<DELTA,TILESIZEY,TILESIZEX>(Ashmem,Bshmem,Creg,ty);

  /* swap results */
  if (N_UNROLL<6)
    _dev_swap<TILESIZEY,TILESIZEX>(Ashmem,Bshmem,Creg,ty);
}


template<class TYPE, class VTYPE, const int N_UNROLL, const int DELTA>
__device__ void multiplyStriped(TYPE * __restrict__ A, TYPE * __restrict__ B, VTYPE* Creg,
				VTYPE Ashmem[TILESIZE][TILESIZEY],
				VTYPE Bshmem[TILESIZE][TILESIZEX]) {

  /* prefetch the first tiles */
  VTYPE Areg = _dev_loadA<TYPE,VTYPE>(A);
  VTYPE Breg = _dev_loadB<TYPE,VTYPE>(B);  

  int ty = (threadIdx.y>>1) % 2;
  
  /* loop over the interior tiles */
  for (int n=1; n<=dev_N; ++n) {
    /* transfer A registers to shmem */
    Ashmem[threadIdx.y][threadIdx.x] = Areg;
    Bshmem[threadIdx.x][threadIdx.y] = Breg;    
    __syncthreads();

    /* load the next tiles */        
    A+=TILESIZE * dev_nRowsAPadded;
    if (n<dev_N || threadIdx.x<DELTA) Areg = _dev_loadA<TYPE,VTYPE>(A);    

    B+=BSTRIDE;
    if (n<dev_N || threadIdx.y<DELTA) Breg = _dev_loadB<TYPE,VTYPE>(B);  

    /* compute the dot products */
    _dev_dotproducts<TILESIZEY,TILESIZEX>(Ashmem,Bshmem,Creg,ty);
    __syncthreads();
  }

  /* transfer A registers to shmem */
  Ashmem[threadIdx.y][threadIdx.x] = Areg;
  Bshmem[threadIdx.x][threadIdx.y] = Breg;
  __syncthreads();  
  
  /* compute the dot products */
  _dev_dotproductsF<DELTA,TILESIZEY,TILESIZEX>(Ashmem,Bshmem,Creg,ty);

  /* swap results */
  if (N_UNROLL<6)
    _dev_swap<TILESIZEY,TILESIZEX>(Ashmem,Bshmem,Creg,ty);
}



template<class TYPE, const int N_UNROLL, const int DELTA>
__device__ void multiplyNew(TYPE * __restrict__ A, TYPE * __restrict__ B, TYPE* Creg,
			    TYPE Ashmem[N_UNROLL*TILESIZEY][TILESIZEX],
			    TYPE Bshmem[N_UNROLL*TILESIZEY][TILESIZEX]) {

  /* prefetch the first tiles */
  TYPE Areg[N_UNROLL];
  TYPE Breg[N_UNROLL];
  _dev_loadA<TYPE,N_UNROLL>(A,Areg);
  _dev_loadB<TYPE,N_UNROLL>(B,Breg);

  /* loop over the interior tiles */
  for (int n=1; n<=dev_N; ++n) {
    /* transfer A registers to shmem */
    for (int i=0; i<N_UNROLL; ++i) {
      //threadIdx.y+i*TILESIZEY
      //Ashmem[threadIdx.y*N_UNROLL+i][threadIdx.x] = Areg[i];
      //Bshmem[threadIdx.x*N_UNROLL+i][threadIdx.y] = Breg[i];

      Ashmem[threadIdx.y+i*TILESIZE][threadIdx.x] = Areg[i];
      Bshmem[threadIdx.x+i*TILESIZE][threadIdx.y] = Breg[i];
    }
    __syncthreads();
    
    /* load the next A tiles */
    A+=TILESIZE;
    B+=TILESIZE*dev_nColsB;
    if (n<dev_N || threadIdx.x<DELTA) _dev_loadA<TYPE,N_UNROLL>(A,Areg);
    if (n<dev_N || threadIdx.y<DELTA) _dev_loadB<TYPE,N_UNROLL>(B,Breg);  

    /* compute the dot products */
    TYPE t1, t2;
#pragma unroll
    for (int m=0; m<N_UNROLL; ++m) {
#pragma unroll
      for (int p=0; p<N_UNROLL; ++p) {
	int i = m*N_UNROLL + p;
#pragma unroll
	for (int k=0; k<TILESIZE; ++k) {
	  //t1 = Ashmem[threadIdx.y*N_UNROLL + m][k];
	  //t2 = Bshmem[threadIdx.x*N_UNROLL + p][k];
	  t1 = Ashmem[threadIdx.y + m*TILESIZE][k];
	  t2 = Bshmem[threadIdx.x + p*TILESIZE][k];
	  Creg[i] = fma(t1,t2,Creg[i]);
	}
      }
    }
    __syncthreads();
  }

  /* transfer registers to shmem */
  for (int i=0; i<N_UNROLL; ++i) {
    //threadIdx.y+i*TILESIZEY
    Ashmem[threadIdx.y+i*TILESIZE][threadIdx.x] = Areg[i];
    Bshmem[threadIdx.x+i*TILESIZE][threadIdx.y] = Breg[i];
    //Ashmem[threadIdx.y*N_UNROLL+i][threadIdx.x] = Areg[i];
    //Bshmem[threadIdx.x*N_UNROLL+i][threadIdx.y] = Breg[i];
  }
  __syncthreads();
  
  /* compute the dot products */
  TYPE t1, t2;
#pragma unroll
  for (int m=0; m<N_UNROLL; ++m) {
#pragma unroll
    for (int n=0; n<N_UNROLL; ++n) {
      int i = m*N_UNROLL + n;
#pragma unroll
      for (int k=0; k<DELTA; ++k) {
	//t1 = Ashmem[threadIdx.y*N_UNROLL + m][k];
	//t2 = Bshmem[threadIdx.x*N_UNROLL + n][k];
	t1 = Ashmem[threadIdx.y + m*TILESIZE][k];
	t2 = Bshmem[threadIdx.x + n*TILESIZE][k];
	Creg[i] = fma(t1,t2,Creg[i]);
      }
    }
  }
}


template<class TYPE, class VTYPE, const int NREGS>
__device__ void multiply2(const int tidx, TYPE * __restrict__ A, TYPE * __restrict__ B,
			  VTYPE* Creg, VTYPE Ashmem[TILESIZE][TILESIZE]) {

  int n, i;

  /* allocate and initialize registers */
  VTYPE Areg[NREGS];
  TYPE Breg;
  
  /* prefetch the first tiles */
  for (i=0; i<NREGS; ++i) {
    int ir = (i*blockDim.y+threadIdx.y)*dev_nColsA;
    Areg[i].x = A[ir];
    Areg[i].y = A[ir+TILESIZE*dev_nColsA];
  }
  
  /* loop over the interior tiles */
  for (n=0; n<dev_N; ++n) {
    /* transfer A registers to shmem */
    for (i=0; i<NREGS; ++i)
      Ashmem[threadIdx.y+i*blockDim.y][threadIdx.x] = Areg[i];
    __syncthreads();

    A+=TILESIZE;
    for (i=0; i<NREGS; ++i) {
      int ir = (i*blockDim.y+threadIdx.y)*dev_nColsA;
      Areg[i].x = A[ir];
      Areg[i].y = A[ir+TILESIZE*dev_nColsA];
    }

    /* outer products */    
#pragma unroll
    for (i=0; i<TILESIZE; ++i) {
      Breg = B[0];      
      for (int j=0; j<16; ++j) {
	Creg[j].x += Breg * Ashmem[j][i].x;
	Creg[j].y += Breg * Ashmem[j][i].y;
      }
      B += dev_nColsB;
    }
    __syncthreads();
  }
  
  /* load the last tiles from A */
  for (i=0; i<NREGS; ++i) {
    int ir = (i*blockDim.y+threadIdx.y)*dev_nColsA;
    if (threadIdx.x<dev_DELTA) {
      Areg[i].x = A[ir];
      Areg[i].y = A[ir+TILESIZE*dev_nColsA];
    } else {
      Areg[i].x = 0.;
      Areg[i].y = 0.;
    }
  }

  /* transfer registers to shmem */
  for (i=0; i<NREGS; ++i)
    Ashmem[threadIdx.y+i*blockDim.y][threadIdx.x] = Areg[i];
  __syncthreads();
  
  /* outer products */    
#pragma unroll
  for (i=0; i<dev_DELTA; ++i) {
    Breg = B[0];
    for (int j=0; j<16; ++j) {
      Creg[j].x += Breg * Ashmem[j][i].x;
      Creg[j].y += Breg * Ashmem[j][i].y;
    }
    B += dev_nColsB;
  }
}

#endif /* !defined(_MULTIPLY_HCU_) */
